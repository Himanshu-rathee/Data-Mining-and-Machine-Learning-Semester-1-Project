---
title: "Credit_card_fraud"
output:
  word_document: default
  html_document: default
---
# Part 1 : Decision Tree Modelling
## STEP 1 : Collecting Data
```{r}
setwd("W:/DMML1/DMML_1_Project/DataSets/D4_credit_fraud")
credit_card <- read.csv("creditcard.csv")
summary(credit_card)

```


## STEP 2 : Exploring and Preparing the Data
### STEP 2.1 : Checking the structure and NA values
```{r}
str(credit_card)
print(paste0("Total NA values in the dataset: ", sum(is.na(credit_card))))

```
### STEP 2.2 : Cleaning the Data


```{r}
#Discretizing the feature variable i.e. class
credit_card$Class <- as.factor(credit_card$Class)
str(credit_card$Class)
#Seeing how many fraud transactions are there in the dataset
print(paste0("Distribution of Normal Tx (0) and Fraud Tx (1)"))
table(credit_card$Class)
```

### STEP 2.3 : Checking the Class ( predictor ) variable

```{r}

#Plotting the classification of transactions to check the dataset
plot(credit_card$Class,main= "Classification of Transactions",ylab="Count",xlab="IS FRAUD?",col="yellow")

```

### STEP 2.4 : Data Preparation (Creating Random Training and Test datasets)

```{r}
library(caret)
index <- createDataPartition(y=credit_card$Class,p=0.75,list = FALSE)
training <- credit_card[index,]
testing <- credit_card[-index,]
dim(training)
dim(testing)
prop.table(table(training$Class))
prop.table(table(testing$Class))
```


```{r}
print("Distribution of normal and fraud transaction in training and testing dataset is:")
as.data.frame(table(training$Class))
as.data.frame(table(testing$Class))
```


### STEP 2.5 : Removing Unbalancing from the Dataset

```{r}
#Setting the seed so that further evalutaions are constant
set.seed(4321)
options(scipen=999)
library(DMwR)
balancedcc <- SMOTE(Class~.,data=training,perc.over = 7500,k=2,perc.under = 300)
print("Distribution of normal and fraud transactions in the training dataset after oversampling is :")
as.data.frame(table(balancedcc$Class))

```



## STEP 3 : Training a Model on the data

```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(e1071)
set.seed(123)
control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
ccf_fit <- train(Class ~., data = training, method = "rpart",
                   parms = list(split = "information"),
                   trControl=control,
                   tuneLength = 10)

```

### STEP 3.1 : Checking the Trained Model

```{r}

ccf_fit
prp(ccf_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```


## STEP 4 : Evaluating Model Performance

```{r}
cc_pred <- predict(ccf_fit,testing)
confusionMatrix(cc_pred,testing$Class)

```


```{r}
library(ROCR)
predd <- prediction(as.numeric(cc_pred), testing$Class)
perfd <- performance(predd, 'tpr','fpr')
plot(perfd, colorize = TRUE, text.adj = c(-0.2,1.7),main="ROC Curve for Decision Tree Model")
abline(a=0,b=1,lwd=2,lty=2,col="grey")
```



# PART 2 : SUPPORT VECTOR MACHINE MODELLING
## STEP 1 till STEP 2.3 are as above used for Decision Tree Modelling
## STEP 2.4 : 

```{r}
library(caret)
library(kernlab)
set.seed(1404)
svm_ccf <- ksvm(Class~.,data=training,kernel="rbfdot")
svm_ccf

```


## STEP 4 : Evalutaing Model 
```{r}
svm_ccf

```

## STEP 5 : 

```{r}
ccf_prediction <- predict(svm_ccf,testing)
confusionMatrix(ccf_prediction,testing$Class)
```

## STEP 6 : Improving Model Performance
## STEP 6.1 : Training SVM model with Kernel package for comparison
```{r}
library(kernlab)
svm_ccf1 <- ksvm(Class~.,data=training,kernel="vanilladot")
svm_ccf1
```

## STEP 6.2 : Evaluating Kernel Model

```{r}
ccf_prediction1 <- predict(svm_ccf1,testing)
confusionMatrix(ccf_prediction1,testing$Class)
```

### STEP 6.3: ROC curve for SVM best model

```{r}
library(ROCR)
preds <- prediction(as.numeric(ccf_prediction1), testing$Class)
perfs <- performance(preds, 'tpr','fpr')
plot(perfs, colorize = TRUE, text.adj = c(-0.2,1.7),main="ROC Curve for SVM Tree Model")
abline(a=0,b=1,lwd=2,lty=2,col="grey")
```


### STEP 6.4: Comparing both the models

```{r}
plot(perfd, col=1, lwd=3,avg= "threshold", main="ROC curve Decision Tree Vs SVM")
plot(perfs, col=2, lwd=3, add=TRUE)
legend(0.7, 0.7, c("Decision Tree","SVM"), 1:2)
abline(a=0,b=1,lwd=2,lty=2,col="grey")
```

